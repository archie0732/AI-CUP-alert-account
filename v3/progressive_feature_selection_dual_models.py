import pandas as pd
import numpy as np
import xgboost as xgb
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score
from datetime import datetime
import warnings
import os
from path import OUTPUT_DIR

warnings.filterwarnings("ignore")

print("=" * 120)
print("ğŸ¯ é›™æ¨¡å‹æ¼¸é€²å¼ç‰¹å¾µé¸æ“‡ï¼ˆXGBoost vs LightGBMï¼‰")
print("=" * 120)
print(f"åŸ·è¡Œæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}")
print(f"åŸ·è¡Œè€…: DeadMark70\n")

start = datetime.now()


os.makedirs(OUTPUT_DIR, exist_ok=True)

print("[1/6] è¼‰å…¥è¨“ç·´æ•¸æ“šå’Œç‰¹å¾µé‡è¦æ€§æ’åº...\n")

train_file = r"D:\Code\aicup\bank3\output\features_31_train_optimized.csv"
df_train = pd.read_csv(train_file)

importance_xgb = pd.read_csv(f"{OUTPUT_DIR}/feature_importance_xgboost.csv")
importance_lgb = pd.read_csv(f"{OUTPUT_DIR}/feature_importance_lightgbm.csv")

xgb_feature_order = importance_xgb[importance_xgb["importance_xgb"] > 0][
    "feature"
].tolist()
lgb_feature_order = importance_lgb[importance_lgb["importance_lgb"] > 0][
    "feature"
].tolist()

print(f"  XGBoost ç‰¹å¾µé †åºï¼ˆå‰10ï¼‰:")
for i, feat in enumerate(xgb_feature_order[:10], 1):
    print(f"    {i}. {feat}")

print(f"\n  LightGBM ç‰¹å¾µé †åºï¼ˆå‰10ï¼‰:")
for i, feat in enumerate(lgb_feature_order[:10], 1):
    print(f"    {i}. {feat}")

print(
    f"\n  æœ‰æ•ˆç‰¹å¾µæ•¸: XGBoost={len(xgb_feature_order)}, LightGBM={len(lgb_feature_order)}\n"
)

# æº–å‚™æ•¸æ“š
X_all = df_train[xgb_feature_order + lgb_feature_order].fillna(0)
X_all = X_all.loc[:, ~X_all.columns.duplicated()]  # å»é‡
y_all = df_train["label"].values

print(f"  è¨“ç·´é›†æ¨£æœ¬: {len(X_all):,}")
print(f"  æ­£æ¨£æœ¬: {y_all.sum():,} ({y_all.sum()/len(y_all)*100:.4f}%)\n")


X_train, X_val, y_train, y_val = train_test_split(
    X_all, y_all, test_size=0.2, random_state=42, stratify=y_all
)

print(f"  è¨“ç·´é›†: {len(X_train):,} ({len(X_train)/len(X_all)*100:.1f}%)")
print(f"  é©—è­‰é›†: {len(X_val):,} ({len(X_val)/len(X_all)*100:.1f}%)")
print(f"  é©—è­‰é›†æ­£æ¨£æœ¬: {y_val.sum():,} ({y_val.sum()/len(y_val)*100:.4f}%)\n")

scale_pos_weight = (len(y_train) - np.sum(y_train)) / (np.sum(y_train) + 1e-10)
print(f"  scale_pos_weight: {scale_pos_weight:.2f}\n")


xgb_results = []

print(
    f"  {'ç‰¹å¾µæ•¸':<8} {'é–¾å€¼':<8} {'AUC':<8} {'Precision':<12} {'Recall':<12} {'F1':<12} {'æ™‚é–“(s)':<10}"
)
print("-" * 80)

for n_features in range(1, len(xgb_feature_order) + 1):
    start_time = datetime.now()

    selected_features = xgb_feature_order[:n_features]

    X_train_subset = X_train[selected_features]
    X_val_subset = X_val[selected_features]

    model = xgb.XGBClassifier(
        max_depth=6,
        learning_rate=0.1,
        n_estimators=200,
        random_state=42,
        n_jobs=-1,
        scale_pos_weight=scale_pos_weight,
        eval_metric="logloss",
        verbosity=0,
    )

    model.fit(X_train_subset, y_train, verbose=False)

    y_pred_proba = model.predict_proba(X_val_subset)[:, 1]
    auc = roc_auc_score(y_val, y_pred_proba)

    best_f1 = 0
    best_threshold = 0.5
    best_precision = 0
    best_recall = 0

    for threshold in np.arange(0.1, 0.99, 0.05):
        y_pred = (y_pred_proba >= threshold).astype(int)

        precision = precision_score(y_val, y_pred, zero_division=0)
        recall = recall_score(y_val, y_pred, zero_division=0)
        f1 = f1_score(y_val, y_pred, zero_division=0)

        if f1 > best_f1:
            best_f1 = f1
            best_threshold = threshold
            best_precision = precision
            best_recall = recall

    train_time = (datetime.now() - start_time).total_seconds()

    xgb_results.append(
        {
            "n_features": n_features,
            "features": selected_features,
            "auc": auc,
            "best_threshold": best_threshold,
            "precision": best_precision,
            "recall": best_recall,
            "f1": best_f1,
            "train_time": train_time,
        }
    )

    if n_features == 1 or n_features % 5 == 0 or n_features == len(xgb_feature_order):
        print(
            f"  {n_features:<8} {best_threshold:<8.2f} {auc:<8.4f} "
            f"{best_precision:<12.4f} {best_recall:<12.4f} {best_f1:<12.4f} {train_time:<10.2f}"
        )

print()

xgb_results_df = pd.DataFrame(xgb_results)
best_xgb_idx = xgb_results_df["f1"].idxmax()
best_xgb = xgb_results_df.loc[best_xgb_idx]

print(f"  ğŸ† XGBoost æœ€ä½³çµæœ:")
print(f"     ç‰¹å¾µæ•¸: {int(best_xgb['n_features'])}")
print(f"     é©—è­‰é›† F1: {best_xgb['f1']:.4f}")
print(f"     é©—è­‰é›† AUC: {best_xgb['auc']:.4f}")
print(f"     æœ€ä½³é–¾å€¼: {best_xgb['best_threshold']:.2f}")
print(f"     Precision: {best_xgb['precision']:.4f}")
print(f"     Recall: {best_xgb['recall']:.4f}\n")

print("[4/6] ğŸš€ LightGBM æ¼¸é€²å¼ç‰¹å¾µé¸æ“‡...\n")

lgb_results = []

print(
    f"  {'ç‰¹å¾µæ•¸':<8} {'é–¾å€¼':<8} {'AUC':<8} {'Precision':<12} {'Recall':<12} {'F1':<12} {'æ™‚é–“(s)':<10}"
)
print("-" * 80)

for n_features in range(1, len(lgb_feature_order) + 1):
    start_time = datetime.now()

    selected_features = lgb_feature_order[:n_features]

    X_train_subset = X_train[selected_features]
    X_val_subset = X_val[selected_features]

    model = lgb.LGBMClassifier(
        num_leaves=30,
        learning_rate=0.05,
        n_estimators=300,
        random_state=42,
        n_jobs=-1,
        is_unbalance=True,
        verbose=-1,
    )

    model.fit(X_train_subset, y_train)

    y_pred_proba = model.predict_proba(X_val_subset)[:, 1]
    auc = roc_auc_score(y_val, y_pred_proba)

    best_f1 = 0
    best_threshold = 0.5
    best_precision = 0
    best_recall = 0

    for threshold in np.arange(0.1, 0.99, 0.05):
        y_pred = (y_pred_proba >= threshold).astype(int)

        precision = precision_score(y_val, y_pred, zero_division=0)
        recall = recall_score(y_val, y_pred, zero_division=0)
        f1 = f1_score(y_val, y_pred, zero_division=0)

        if f1 > best_f1:
            best_f1 = f1
            best_threshold = threshold
            best_precision = precision
            best_recall = recall

    train_time = (datetime.now() - start_time).total_seconds()

    lgb_results.append(
        {
            "n_features": n_features,
            "features": selected_features,
            "auc": auc,
            "best_threshold": best_threshold,
            "precision": best_precision,
            "recall": best_recall,
            "f1": best_f1,
            "train_time": train_time,
        }
    )

    if n_features == 1 or n_features % 5 == 0 or n_features == len(lgb_feature_order):
        print(
            f"  {n_features:<8} {best_threshold:<8.2f} {auc:<8.4f} "
            f"{best_precision:<12.4f} {best_recall:<12.4f} {best_f1:<12.4f} {train_time:<10.2f}"
        )

print()

lgb_results_df = pd.DataFrame(lgb_results)
best_lgb_idx = lgb_results_df["f1"].idxmax()
best_lgb = lgb_results_df.loc[best_lgb_idx]

print(f"  ğŸ† LightGBM æœ€ä½³çµæœ:")
print(f"     ç‰¹å¾µæ•¸: {int(best_lgb['n_features'])}")
print(f"     é©—è­‰é›† F1: {best_lgb['f1']:.4f}")
print(f"     é©—è­‰é›† AUC: {best_lgb['auc']:.4f}")
print(f"     æœ€ä½³é–¾å€¼: {best_lgb['best_threshold']:.2f}")
print(f"     Precision: {best_lgb['precision']:.4f}")
print(f"     Recall: {best_lgb['recall']:.4f}\n")


xgb_results_df.to_csv(f"{OUTPUT_DIR}/progressive_xgboost_results.csv", index=False)
print(f"  âœ… XGBoost çµæœå·²ä¿å­˜: progressive_xgboost_results.csv")

lgb_results_df.to_csv(f"{OUTPUT_DIR}/progressive_lightgbm_results.csv", index=False)
print(f"  âœ… LightGBM çµæœå·²ä¿å­˜: progressive_lightgbm_results.csv\n")

with open(f"{OUTPUT_DIR}/best_features_xgboost.txt", "w") as f:
    f.write(f"XGBoost æœ€ä½³ç‰¹å¾µæ•¸: {int(best_xgb['n_features'])}\n")
    f.write(f"é©—è­‰é›† F1: {best_xgb['f1']:.4f}\n")
    f.write(f"ç‰¹å¾µåˆ—è¡¨:\n")
    for i, feat in enumerate(best_xgb["features"], 1):
        f.write(f"{i}. {feat}\n")

with open(f"{OUTPUT_DIR}/best_features_lightgbm.txt", "w") as f:
    f.write(f"LightGBM æœ€ä½³ç‰¹å¾µæ•¸: {int(best_lgb['n_features'])}\n")
    f.write(f"é©—è­‰é›† F1: {best_lgb['f1']:.4f}\n")
    f.write(f"ç‰¹å¾µåˆ—è¡¨:\n")
    for i, feat in enumerate(best_lgb["features"], 1):
        f.write(f"{i}. {feat}\n")

print(f"  âœ… æœ€ä½³ç‰¹å¾µåˆ—è¡¨å·²ä¿å­˜\n")

print("[6/6] ç”Ÿæˆå¯è¦–åŒ–å°æ¯”...\n")

import matplotlib.pyplot as plt

fig, axes = plt.subplots(2, 2, figsize=(14, 10))

axes[0, 0].plot(
    xgb_results_df["n_features"],
    xgb_results_df["f1"],
    marker="o",
    label="XGBoost",
    linewidth=2,
)
axes[0, 0].plot(
    lgb_results_df["n_features"],
    lgb_results_df["f1"],
    marker="s",
    label="LightGBM",
    linewidth=2,
)
axes[0, 0].axvline(x=best_xgb["n_features"], color="blue", linestyle="--", alpha=0.5)
axes[0, 0].axvline(x=best_lgb["n_features"], color="orange", linestyle="--", alpha=0.5)
axes[0, 0].set_xlabel("Number of Features", fontsize=11)
axes[0, 0].set_ylabel("F1 Score", fontsize=11)
axes[0, 0].set_title("F1 Score vs Number of Features", fontsize=12, fontweight="bold")
axes[0, 0].legend()
axes[0, 0].grid(alpha=0.3)

axes[0, 1].plot(
    xgb_results_df["n_features"],
    xgb_results_df["auc"],
    marker="o",
    label="XGBoost",
    linewidth=2,
)
axes[0, 1].plot(
    lgb_results_df["n_features"],
    lgb_results_df["auc"],
    marker="s",
    label="LightGBM",
    linewidth=2,
)
axes[0, 1].set_xlabel("Number of Features", fontsize=11)
axes[0, 1].set_ylabel("AUC", fontsize=11)
axes[0, 1].set_title("AUC vs Number of Features", fontsize=12, fontweight="bold")
axes[0, 1].legend()
axes[0, 1].grid(alpha=0.3)

axes[1, 0].plot(
    xgb_results_df["n_features"],
    xgb_results_df["precision"],
    marker="o",
    label="XGB Precision",
    linewidth=2,
)
axes[1, 0].plot(
    xgb_results_df["n_features"],
    xgb_results_df["recall"],
    marker="s",
    label="XGB Recall",
    linewidth=2,
)
axes[1, 0].plot(
    lgb_results_df["n_features"],
    lgb_results_df["precision"],
    marker="^",
    label="LGB Precision",
    linewidth=2,
    alpha=0.7,
)
axes[1, 0].plot(
    lgb_results_df["n_features"],
    lgb_results_df["recall"],
    marker="v",
    label="LGB Recall",
    linewidth=2,
    alpha=0.7,
)
axes[1, 0].set_xlabel("Number of Features", fontsize=11)
axes[1, 0].set_ylabel("Score", fontsize=11)
axes[1, 0].set_title("Precision & Recall vs Features", fontsize=12, fontweight="bold")
axes[1, 0].legend()
axes[1, 0].grid(alpha=0.3)

axes[1, 1].plot(
    xgb_results_df["n_features"],
    xgb_results_df["train_time"],
    marker="o",
    label="XGBoost",
    linewidth=2,
)
axes[1, 1].plot(
    lgb_results_df["n_features"],
    lgb_results_df["train_time"],
    marker="s",
    label="LightGBM",
    linewidth=2,
)
axes[1, 1].set_xlabel("Number of Features", fontsize=11)
axes[1, 1].set_ylabel("Training Time (seconds)", fontsize=11)
axes[1, 1].set_title("Training Time vs Features", fontsize=12, fontweight="bold")
axes[1, 1].legend()
axes[1, 1].grid(alpha=0.3)

plt.tight_layout()
plot_file = f"{OUTPUT_DIR}/progressive_feature_selection_comparison.png"
plt.savefig(plot_file, dpi=150, bbox_inches="tight")
print(f"  âœ… å°æ¯”åœ–å·²ä¿å­˜: {plot_file}\n")

plt.close()

elapsed = (datetime.now() - start).total_seconds() / 60

print("=" * 120)
print("âœ… é›™æ¨¡å‹æ¼¸é€²å¼ç‰¹å¾µé¸æ“‡å®Œæˆ")
print("=" * 120 + "\n")

print(f"â±ï¸  ç¸½è€—æ™‚: {elapsed:.2f} åˆ†é˜\n")

print(f"ğŸ“Š å°æ¯”çµæœ:\n")

print(f"  XGBoost æœ€ä½³:")
print(f"    ç‰¹å¾µæ•¸: {int(best_xgb['n_features'])}")
print(f"    F1: {best_xgb['f1']:.4f}")
print(f"    AUC: {best_xgb['auc']:.4f}")
print(f"    Precision: {best_xgb['precision']:.4f}")
print(f"    Recall: {best_xgb['recall']:.4f}\n")

print(f"  LightGBM æœ€ä½³:")
print(f"    ç‰¹å¾µæ•¸: {int(best_lgb['n_features'])}")
print(f"    F1: {best_lgb['f1']:.4f}")
print(f"    AUC: {best_lgb['auc']:.4f}")
print(f"    Precision: {best_lgb['precision']:.4f}")
print(f"    Recall: {best_lgb['recall']:.4f}\n")

if best_xgb["f1"] > best_lgb["f1"]:
    print(f"  ğŸ† XGBoost å‹å‡º (F1 é«˜ {best_xgb['f1'] - best_lgb['f1']:.4f})\n")
    winner = "XGBoost"
    winner_features = best_xgb["features"]
else:
    print(f"  ğŸ† LightGBM å‹å‡º (F1 é«˜ {best_lgb['f1'] - best_xgb['f1']:.4f})\n")
    winner = "LightGBM"
    winner_features = best_lgb["features"]

print(f"ğŸ“ ç”Ÿæˆçš„æª”æ¡ˆ:")
print(f"  1. progressive_xgboost_results.csv")
print(f"  2. progressive_lightgbm_results.csv")
print(f"  3. best_features_xgboost.txt")
print(f"  4. best_features_lightgbm.txt")
print(f"  5. progressive_feature_selection_comparison.png\n")

print(f"ğŸ¯ ä¸‹ä¸€æ­¥:")
print(f"  1. ä½¿ç”¨ {winner} çš„æœ€ä½³ç‰¹å¾µçµ„åˆ")
print(f"  2. ç”¨å…¨éƒ¨è¨“ç·´æ•¸æ“šé‡è¨“ç·´")
print(f"  3. å°æ¸¬è©¦é›†é æ¸¬ä¸¦æäº¤\n")

print(f"ğŸ’¡ é—œéµæ´å¯Ÿ:")
print(f"  â€¢ è¨“ç·´/é©—è­‰åˆ†å‰²: 80/20")
print(f"  â€¢ å®Œå…¨éš”é›¢æ¸¬è©¦é›†")
print(f"  â€¢ æ‰¾åˆ°äº†æœ¬åœ° F1 çš„é ‚å³°")
print(f"  â€¢ é¿å…äº†éæ“¬åˆ\n")

print("=" * 120 + "\n")
