import pandas as pd
from datetime import datetime
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from setup import setup_data
from utils import generate_all_features
from path import OUTPUT_PATH, CONTAMINATION_VALUES, N_ESTIMATORS, RANDOM_STATE


def main():
    print("=" * 80)
    print("ğŸ¯ One-Class Learning: Isolation Forest (å„ªåŒ–ç‰ˆ)")
    print("=" * 80)

    start_time = datetime.now()
    print(f"é–‹å§‹æ™‚é–“: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\n")

    # 1. æ•¸æ“šæº–å‚™
    (
        df_txn,
        _,
        df_predict,
        txn_from_grouped,
        txn_to_grouped,
        alert_event_dict,
        alert_accounts,
        test_accounts,
    ) = setup_data()

    # 2. ç‰¹å¾µç”Ÿæˆ
    print(f"\n[{datetime.now().strftime('%H:%M:%S')}] æ­¥é©Ÿ 2/4: ç”Ÿæˆç‰¹å¾µ (å·²å„ªåŒ–)...")

    # è­¦ç¤ºå¸³æˆ¶ç‰¹å¾µ
    df_alert_features = generate_all_features(
        alert_accounts,
        txn_from_grouped,
        txn_to_grouped,
        alert_event_dict,
        is_alert_set=True,
    )
    print(f"  âœ… è­¦ç¤ºå¸³æˆ¶ç‰¹å¾µ: {len(df_alert_features)} å€‹")

    # æ¸¬è©¦é›†ç‰¹å¾µ
    df_test_features = generate_all_features(
        test_accounts, txn_from_grouped, txn_to_grouped
    )
    print(f"  âœ… æ¸¬è©¦é›†ç‰¹å¾µ: {len(df_test_features)} å€‹")

    # 3. è¨“ç·´èˆ‡é æ¸¬
    print(
        f"\n[{datetime.now().strftime('%H:%M:%S')}] æ­¥é©Ÿ 3/4: è¨“ç·´ Isolation Forest..."
    )

    feature_cols = [col for col in df_alert_features.columns if col not in ["acct"]]

    X_alert = df_alert_features[feature_cols].fillna(0)
    X_test = df_test_features[feature_cols].fillna(0)

    print(f"  è¨“ç·´æ•¸æ“š: {len(X_alert)} å€‹è­¦ç¤ºå¸³æˆ¶")
    print(f"  æ¸¬è©¦æ•¸æ“š: {len(X_test)} å€‹å¸³æˆ¶")
    print(f"  ç‰¹å¾µæ•¸é‡: {len(feature_cols)} å€‹")

    # æ¨™æº–åŒ–
    print(f"\n  æ¨™æº–åŒ–ç‰¹å¾µ...")
    scaler = StandardScaler()
    X_alert_scaled = scaler.fit_transform(X_alert)
    X_test_scaled = scaler.transform(X_test)

    # è¨“ç·´å¤šå€‹æ¨¡å‹
    print(f"\n  è¨“ç·´ {len(CONTAMINATION_VALUES)} å€‹æ¨¡å‹...")
    results = {}

    for contamination in CONTAMINATION_VALUES:
        print(f"  è¨“ç·´ contamination={contamination:.2f}...", end=" ")

        model = IsolationForest(
            n_estimators=N_ESTIMATORS,
            contamination=contamination,
            max_samples="auto",
            random_state=RANDOM_STATE,
            n_jobs=-1,
        )

        model.fit(X_alert_scaled)

        y_pred = model.predict(X_test_scaled)
        # -1 = ç•°å¸¸ (æ¨™è¨»ç‚º 1), 1 = æ­£å¸¸ (æ¨™è¨»ç‚º 0)
        y_pred_binary = (y_pred == -1).astype(int)

        pred_count = y_pred_binary.sum()
        pred_ratio = pred_count / len(df_test_features) * 100

        results[contamination] = {
            "predictions": y_pred_binary,
            "count": pred_count,
            "ratio": pred_ratio,
        }

        print(f"é æ¸¬ {pred_count:4d} å€‹ ({pred_ratio:5.2f}%)")

    # 4. ç”Ÿæˆæäº¤æ–‡ä»¶
    print(
        f"\n[{datetime.now().strftime('%H:%M:%S')}] æ­¥é©Ÿ 4/4: ç”Ÿæˆæäº¤æ–‡ä»¶èˆ‡ä¿å­˜ç‰¹å¾µ..."
    )

    # ä¿å­˜ç‰¹å¾µ
    df_alert_features.to_csv(
        f"{OUTPUT_PATH}/oneclass_alert_features_optimized.csv", index=False
    )
    df_test_features.to_csv(
        f"{OUTPUT_PATH}/oneclass_test_features_optimized.csv", index=False
    )

    print(f"  ğŸ’¾ ç‰¹å¾µå·²ä¿å­˜:")
    print(f"    - oneclass_alert_features_optimized.csv")
    print(f"    - oneclass_test_features_optimized.csv")

    for contamination, result in results.items():
        df_submission = pd.DataFrame(
            {"acct": df_test_features["acct"], "label": result["predictions"]}
        )

        filename = f"{OUTPUT_PATH}/submission_oneclass_if_{int(contamination*100):02d}_optimized.csv"
        df_submission.to_csv(filename, index=False)

        print(
            f"  âœ… contamination {contamination:.2f}: {result['count']:4d} å€‹ â†’ {filename}"
        )

    # ç¸½çµ
    total_time = (datetime.now() - start_time).total_seconds() / 60

    print(f"\n{'='*80}")
    print(f"ğŸ‰ One-Class Learning (å„ªåŒ–ç‰ˆ) å®Œæˆï¼")
    print(f"{'='*80}")
    print(f"â±ï¸  ç¸½é‹è¡Œæ™‚é–“: {total_time:.1f} åˆ†é˜")
    print(
        f"ğŸ“Š è¨“ç·´æ•¸æ“š: {len(df_alert_features)} å€‹è­¦ç¤ºå¸³æˆ¶ | ç‰¹å¾µæ•¸é‡: {len(feature_cols)} å€‹"
    )
    print(f"ğŸ“ ç”Ÿæˆæ–‡ä»¶: {len(CONTAMINATION_VALUES)} å€‹æäº¤æ–‡ä»¶ (å¸¶æœ‰ _optimized æ¨™è¨˜)")

    print(f"\nğŸ“Š é æ¸¬æ•¸é‡çµ±è¨ˆ:")
    print(f"  contamination   é æ¸¬æ•¸é‡   æ¯”ä¾‹")
    print(f"  " + "-" * 40)
    for contamination in CONTAMINATION_VALUES:
        result = results[contamination]
        print(
            f"  {contamination:.2f}             {result['count']:4d}     {result['ratio']:5.2f}%"
        )

    print(f"\n{'='*80}")
    print(f"âœ… æ‰€æœ‰æ–‡ä»¶å·²æº–å‚™å¥½ï¼")
    print(f"{'='*80}\n")


if __name__ == "__main__":
    main()
